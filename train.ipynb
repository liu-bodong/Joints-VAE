{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5c9014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "import network\n",
    "import kinematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ad1945",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3f5ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_vae_model = network.fROM_VAE_task(input_dim=3, latent_dim=128, num_task_points=256).to(device)\n",
    "task_vae_model.load_state_dict(torch.load(\"task_vae_76_4096.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db17da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_USERS = 76\n",
    "M_POSES = 4096  # Number of points to sample joint-space\n",
    "K_TASK_POINTS = 256   # Number of points to generate in task-space\n",
    "LATENT_DIM = 128\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 76\n",
    "BETA_KL = 0.001 # Weight for KL loss\n",
    "\n",
    "\n",
    "joint_limits = np.loadtxt(\"joint_limits.csv\", delimiter=',', skiprows=1)\n",
    "joint_limits = torch.tensor(joint_limits, dtype=torch.float32)\n",
    "\n",
    "joint_configs = np.loadtxt(\"joints_data_81_4096.csv\", delimiter=',', skiprows=1)\n",
    "joint_configs = torch.tensor(joint_configs, dtype=torch.float32)\n",
    "\n",
    "task_points   = np.loadtxt(\"task_sp_points.csv\", delimiter=',', skiprows=1)\n",
    "task_points   = torch.tensor(task_points, dtype=torch.float32)\n",
    "\n",
    "DATASET_SIZE = N_USERS\n",
    "TRAIN_SIZE   = math.floor(0.8 * DATASET_SIZE)\n",
    "VAL_SIZE     = DATASET_SIZE - TRAIN_SIZE\n",
    "\n",
    "print(joint_configs.shape)\n",
    "print(task_points.shape)\n",
    "print(TRAIN_SIZE, VAL_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a925c6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_configs = joint_configs[:, 1:]  # Remove user ID column\n",
    "print(joint_configs.shape)\n",
    "\n",
    "task_points_FK = kinematics.forward_kinematics_pytorch(joint_configs)\n",
    "task_points_FK =task_points_FK.view(-1, M_POSES, 3)\n",
    "\n",
    "joint_configs = joint_configs.view(-1, M_POSES, 4)  # [N, M, 4]\n",
    "\n",
    "task_points = task_points[:, 1:]  # Remove user ID column\n",
    "task_points = task_points.view(-1, M_POSES, 3)  # [N, K, 3]\n",
    "\n",
    "\n",
    "# joint_limits = joint_limits[:, 1:]  # Remove user ID column\n",
    "\n",
    "print(joint_limits.shape)\n",
    "print(joint_configs.shape)\n",
    "print(task_points.shape)\n",
    "print(task_points_FK.shape)\n",
    "\n",
    "error_FK = nn.functional.mse_loss(task_points_FK.view(-1, 3), task_points.view(-1,3))\n",
    "error_FK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83b737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(joint_configs, task_points)\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [TRAIN_SIZE, VAL_SIZE])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "joints_encoder = network.Encoder(input_dim=4, latent_dim=LATENT_DIM).to(device)\n",
    "\n",
    "# trained task space vae, freeze the encodeer\n",
    "task_vae_model = network.fROM_VAE_task(input_dim=3, latent_dim=128, num_task_points=256).to(device)\n",
    "task_vae_model.load_state_dict(torch.load(\"task_vae_76_4096.pth\", map_location=device))\n",
    "task_encoder = task_vae_model.encoder\n",
    "task_decoder = task_vae_model.decoder\n",
    "for param in task_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in task_decoder.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in task_vae_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(joints_encoder.parameters(), lr=LEARNING_RATE)\n",
    "recon_loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fd0c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the joint encoder to match the task space representation in the latent space\n",
    "EPOCHS = 10000\n",
    "LEARNING_RATE = 5e-6\n",
    "# joints_encoder.load_state_dict(torch.load(\"joints_encoder.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8253d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the joint encoder to match the task space representation in the latent space\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    joints_encoder.train()\n",
    "    task_encoder.eval()\n",
    "    total_loss_epoch = 0\n",
    "    total_recon_loss_epoch = 0\n",
    "    total_kl_loss_epoch = 0\n",
    "    \n",
    "    for x, y in tqdm.tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False):\n",
    "        # print(x.dtype)\n",
    "        x = x.to(device)  # [B, M, 4]\n",
    "        y = y.to(device)  # [B, M, 3]\n",
    "        \n",
    "        # x_gt = kinematics.forward_kinematics_pytorch(x.view(-1, 4))  # [B * M, 3]\n",
    "        mu_y, logvar_y = task_encoder(y.view(-1, 4096, 3))\n",
    "        var_y = torch.exp(logvar_y)\n",
    "        \n",
    "        # Forward pass\n",
    "        mu_pred, logvar_pred = joints_encoder(x) # [B, 8], [B, D_z], [B, D_z]\n",
    "        var_pred = torch.exp(logvar_pred)\n",
    "        \n",
    "        \n",
    "        # Loss\n",
    "        kl_div = 0.5 * torch.sum(\n",
    "            logvar_y - logvar_pred + (var_pred + (mu_pred - mu_y).pow(2)) / var_y - 1,\n",
    "            dim=1 # Sum over the latent dimensions\n",
    "        )\n",
    "        \n",
    "        # mse_loss = nn.functional.mse_loss(mu_pred, mu_y)\n",
    "                \n",
    "        loss = torch.mean(kl_div)\n",
    "        \n",
    "        if epoch == EPOCHS - 1:\n",
    "            # print(f\"{mu_pred[:3].detach().cpu().numpy()}\")\n",
    "            # print(f\"{mu_y[:3].detach().cpu().numpy()}\")\n",
    "            print(f\"recon_loss: {loss.item():.4f})\")\n",
    "            \n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss_epoch += loss.item()\n",
    "\n",
    "\n",
    "    # Print epoch stats\n",
    "    avg_loss = total_loss_epoch / len(train_loader) \n",
    "    \n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {avg_loss:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae8bdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from network import fROM_VAE_task\n",
    "import utils\n",
    "\n",
    "task_encoder.eval()\n",
    "for param in task_vae_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "recon_error = 0.0\n",
    "latent_error = 0.0\n",
    "for x, y in val_loader:\n",
    "    with torch.no_grad():\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        mu_pred, logvar_pred = joints_encoder(x) # [B, D_z], [B, D_z]\n",
    "        mu_y, logvar_y = task_encoder(y)\n",
    "        \n",
    "        var_y = torch.exp(logvar_y)\n",
    "        var_pred = torch.exp(logvar_pred)\n",
    "        \n",
    "        z_pred = task_vae_model.reparameterize(mu_pred, logvar_pred)\n",
    "        z_y    = task_vae_model.reparameterize(mu_y, logvar_y)\n",
    "        \n",
    "        pc_pred = task_decoder(z_pred)\n",
    "        pc_y    = task_decoder(z_y)\n",
    "        \n",
    "        recon_error += utils.chamfer_loss(pc_pred, pc_y).item()\n",
    "        \n",
    "        kl_div = 0.5 * torch.sum(\n",
    "            logvar_y - logvar_pred + (var_pred + (mu_pred - mu_y).pow(2)) / var_y - 1,\n",
    "            dim=1 # Sum over the latent dimensions\n",
    "        )\n",
    "        \n",
    "        latent_error += torch.mean(kl_div)\n",
    "        \n",
    "        # print(f\"{pc_pred[:3].detach().cpu().numpy()}\")\n",
    "        # print(f\"{pc_y[:3].detach().cpu().numpy()}\")\n",
    "        \n",
    "avg_recon_error = recon_error / len(val_loader)\n",
    "avg_latent_error = latent_error / len(val_loader)\n",
    "print(f\"Chamfer dist: {avg_recon_error:.8f}, Latent KL: {avg_latent_error:.8f}\")\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949c55a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(joints_encoder.state_dict(), 'joints_encoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1200a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "joints_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497f4316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize some reconstructions\n",
    "with torch.no_grad():\n",
    "    x = joint_configs[13, :, :].to(device) # [B, 4]\n",
    "    y = kinematics.forward_kinematics_pytorch(x.view(-1, 4).to(device))\n",
    "    y = y.view(1, -1, 3)\n",
    "    # y = task_points[1, :, :].to(device) # [B, 3]\n",
    "\n",
    "    x = x.unsqueeze(1)\n",
    "    # y = y.unsqueeze(1)\n",
    "\n",
    "    print(1)\n",
    "    mu_pred, logvar_pred = joints_encoder(x) # [B, D_z], [B, D_z]\n",
    "    \n",
    "    print(2)\n",
    "    mu_y, logvar_y = task_encoder(y)\n",
    "\n",
    "    z_pred = task_vae_model.reparameterize(mu_pred, logvar_pred)\n",
    "    z_y    = task_vae_model.reparameterize(mu_y, logvar_y)\n",
    "\n",
    "    pc_pred = task_decoder(z_pred)\n",
    "    pc_y    = task_decoder(z_y)\n",
    "    \n",
    "    print(\"plotting\")\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    pc_pred_np = pc_pred[0].detach().cpu().numpy()\n",
    "    pc_y_np    = pc_y[0].detach().cpu().numpy()\n",
    "\n",
    "    ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    ax1.scatter(pc_pred_np[:, 0], pc_pred_np[:, 1], pc_pred_np[:, 2], s=2, c='r')\n",
    "    ax1.set_title(\"Predicted point cloud\")\n",
    "    ax1.set_xlabel(\"X\"); ax1.set_ylabel(\"Y\"); ax1.set_zlabel(\"Z\")\n",
    "\n",
    "    ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "    ax2.scatter(pc_y_np[:, 0], pc_y_np[:, 1], pc_y_np[:, 2], s=2, c='b')\n",
    "    ax2.set_title(\"Ground-truth point cloud\")\n",
    "    ax2.set_xlabel(\"X\"); ax2.set_ylabel(\"Y\"); ax2.set_zlabel(\"Z\")\n",
    "\n",
    "    all_pts = np.vstack([pc_pred_np, pc_y_np])\n",
    "    mins = all_pts.min(axis=0)\n",
    "    maxs = all_pts.max(axis=0)\n",
    "    for ax in (ax1, ax2):\n",
    "        ax.set_xlim(mins[0], maxs[0])\n",
    "        ax.set_ylim(mins[1], maxs[1])\n",
    "        ax.set_zlim(mins[2], maxs[2])\n",
    "        ax.view_init(elev=20, azim=120)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc0e849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aec54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_test_sample, y_test_sample = next(iter(val_dataset)) # [B, M, 4], [B, 8]\n",
    "    \n",
    "    x_input = x_test_sample[0].unsqueeze(0).to(device) # [1, M, 4]\n",
    "    y_truth = y_test_sample[0].cpu().numpy()          # [8]\n",
    "    \n",
    "    y_pred, _, _ = model(x_input) # [1, 8]\n",
    "    y_pred = y_pred.squeeze().cpu().numpy()\n",
    "    \n",
    "    y_truth_deg = np.rad2deg(y_truth)\n",
    "    y_pred_deg = np.rad2deg(y_pred)\n",
    "\n",
    "    y_truth_deg[0:4] = -1 * (y_truth_deg[0:4] % 360)\n",
    "    y_truth_deg[4:] = y_truth_deg[4:] % 360\n",
    "    y_pred_deg[0:4] = -1 * (y_pred_deg[0:4] % 360)\n",
    "    y_pred_deg[4:] = y_pred_deg[4:] % 360\n",
    "\n",
    "    \n",
    "    # print(\"Example from Test Set (in deg):\")\n",
    "    # print(\"---------------------------------\")\n",
    "    # print(f\"JOINT         | TRUTH (min/max)  | PRED (min/max)\")\n",
    "    # print(f\"Shoulder Abd  | {y_truth_deg[0]:.1f} / {y_truth_deg[4]:.1f}   | {y_pred_deg[0]:.1f} / {y_pred_deg[4]:.1f}\")\n",
    "    # print(f\"Shoulder Flex | {y_truth_deg[1]:.1f} / {y_truth_deg[5]:.1f}  | {y_pred_deg[1]:.1f} / {y_pred_deg[5]:.1f}\")\n",
    "    # print(f\"Shoulder Rot  | {y_truth_deg[2]:.1f} / {y_truth_deg[6]:.1f}   | {y_pred_deg[2]:.1f} / {y_pred_deg[6]:.1f}\")\n",
    "    # print(f\"Elbow Flex    | {y_truth_deg[3]:.1f} / {y_truth_deg[7]:.1f}    | {y_pred_deg[3]:.1f} / {y_pred_deg[7]:.1f}\")\n",
    "    \n",
    "    print(\"Example from Test Set (in deg):\")\n",
    "    print(\"---------------------------------\")\n",
    "    print(f\"JOINT         | TRUTH (min/max)  | PRED (min/max)\")\n",
    "    print(f\"Shoulder Abd  | {y_truth[0]:.1f} / {y_truth[4]:.1f}   | {y_pred[0]:.1f} / {y_pred[4]:.1f}\")\n",
    "    print(f\"Shoulder Flex | {y_truth[1]:.1f} / {y_truth[5]:.1f}   | {y_pred[1]:.1f} / {y_pred[5]:.1f}\")\n",
    "    print(f\"Shoulder Rot  | {y_truth[2]:.1f} / {y_truth[6]:.1f}   | {y_pred[2]:.1f} / {y_pred[6]:.1f}\")\n",
    "    print(f\"Elbow Flex    | {y_truth[3]:.1f} / {y_truth[7]:.1f}   | {y_pred[3]:.1f} / {y_pred[7]:.1f}\")\n",
    "\n",
    "    mse_rad = np.mean((y_truth - y_pred)**2) # MSE in rad\n",
    "    print(f\"\\nExample's MSE (deg^2): {mse_rad:.6f}\")\n",
    "    # mse_deg = np.mean((y_truth_deg - y_pred_deg)**2) # MSE in deg=\n",
    "    # print(f\"Example's MSE (deg^2): {mse_deg:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee3c251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a942b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
